"""
OpenClaw Agent for HAL Reliability Evaluation.

Imitates agents/openai_codex_agent but swaps the execution backend to
OpenClaw's `agent-send` flow via `openclaw agent`.

For taubench: multi-turn loop where each turn sends full context and asks the
model for exactly one TOOL_CALL JSON object.

For GAIA: single invocation where OpenClaw returns a final concise answer.

Requirements:
    - OpenClaw CLI installed: npm install -g openclaw@latest
    - API key(s) set depending on provider:
        OPENAI_API_KEY and/or ANTHROPIC_API_KEY
"""

import json
import subprocess
import shutil
import os
import re
import time
import uuid
from datetime import date
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

# Reliability metric utilities
from hal.utils.fault_injection import FaultInjector
from hal.utils.compliance_checkers import ComplianceMonitor
from hal.utils.taubench_perturbations import (
    TauBenchPerturbator,
    create_taubench_perturbator,
)
from hal.utils.llm_log_analyzer import LLMLogAnalyzer


# Keep onboarding idempotent when auto-onboard is enabled.
_ONBOARDED_PROVIDERS: set[str] = set()
_CONFIGURED_AGENT_IDS: set[str] = set()
_TOOL_POLICY_CONFIGURED_AGENT_IDS: set[str] = set()


# =============================================================================
# ABSTENTION DETECTION (same patterns as tool_calling agent)
# =============================================================================


def _detect_abstention(conversation_history: list, actions_taken: list) -> dict:
    INABILITY_PATTERNS = [
        r"\bi(?:'m| am) (?:not )?(?:able|unable) to\b",
        r"\bi cannot\b",
        r"\bi can't\b",
        r"\bthis is (?:beyond|outside) (?:my|the) (?:capabilities|scope)\b",
        r"\bi don't have (?:the ability|access|permission)\b",
    ]
    UNCERTAINTY_PATTERNS = [
        r"\bi(?:'m| am) not (?:sure|certain)\b",
        r"\bi don't know\b",
        r"\bi'm uncertain\b",
    ]
    REFUSAL_PATTERNS = [
        r"\bi (?:cannot|can't|won't|will not) (?:proceed|continue|complete)\b",
        r"\bunable to (?:proceed|continue|complete)\b",
    ]

    abstention_scores = {"inability": 0.0, "uncertainty": 0.0, "refusal": 0.0}
    evidence = []

    assistant_messages = []
    for msg in conversation_history:
        if isinstance(msg, dict) and msg.get("role") == "assistant":
            content = msg.get("content", "")
            if content:
                assistant_messages.append(
                    content.lower()
                    if isinstance(content, str)
                    else str(content).lower()
                )

    for text in assistant_messages:
        for pattern in INABILITY_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                abstention_scores["inability"] += 1.0
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    start = max(0, match.start() - 30)
                    end = min(len(text), match.end() + 30)
                    evidence.append(f"[inability] ...{text[start:end]}...")
        for pattern in UNCERTAINTY_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                abstention_scores["uncertainty"] += 0.7
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    start = max(0, match.start() - 30)
                    end = min(len(text), match.end() + 30)
                    evidence.append(f"[uncertainty] ...{text[start:end]}...")
        for pattern in REFUSAL_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                abstention_scores["refusal"] += 1.0
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    start = max(0, match.start() - 30)
                    end = min(len(text), match.end() + 30)
                    evidence.append(f"[refusal] ...{text[start:end]}...")

    total_score = sum(abstention_scores.values())
    abstention_strength = min(1.0, total_score / 3.0)
    abstention_type = (
        "none"
        if total_score == 0
        else max(abstention_scores, key=abstention_scores.get)
    )
    abstained = abstention_strength >= 0.3 or any(
        abstention_scores[t] >= 1.0 for t in ["inability", "refusal"]
    )

    return {
        "abstained": abstained,
        "abstention_type": abstention_type,
        "abstention_strength": abstention_strength,
        "evidence": evidence[:5],
        "early_termination": len(actions_taken) <= 2,
        "scores_by_type": abstention_scores,
        "num_assistant_messages": len(assistant_messages),
    }


# =============================================================================
# OPENCLAW SETUP + OUTPUT PARSING
# =============================================================================


def _normalize_provider(provider: Optional[str], model_name: str) -> str:
    if model_name.startswith("anthropic/"):
        return "anthropic"
    if model_name.startswith("openai/"):
        return "openai"

    p = (provider or "").strip().lower()
    if p == "anthopic":
        p = "anthropic"
    if p in {"openai", "anthropic"}:
        return p
    if "claude" in model_name.lower():
        return "anthropic"
    return "openai"


def _normalize_model_name(provider: str, model_name: Optional[str]) -> str:
    raw = (model_name or "").strip()
    if not raw:
        return "openai/gpt-5.1-codex" if provider == "openai" else "anthropic/claude-opus-4-6"
    if "/" in raw:
        return raw
    return f"{provider}/{raw}"


def _parse_mapping_kwarg(value: Any) -> dict[str, str]:
    if isinstance(value, dict):
        return {
            str(k).strip(): str(v).strip()
            for k, v in value.items()
            if str(k).strip() and str(v).strip()
        }
    if isinstance(value, str) and value.strip():
        try:
            parsed = json.loads(value)
            if isinstance(parsed, dict):
                return {
                    str(k).strip(): str(v).strip()
                    for k, v in parsed.items()
                    if str(k).strip() and str(v).strip()
                }
        except json.JSONDecodeError:
            return {}
    return {}


def _as_bool(value: Any, default: bool) -> bool:
    if value is None:
        return default
    if isinstance(value, bool):
        return value
    text = str(value).strip().lower()
    if text in {"1", "true", "yes", "y", "on"}:
        return True
    if text in {"0", "false", "no", "n", "off"}:
        return False
    return default


def _sanitize_agent_id_component(text: str) -> str:
    cleaned = re.sub(r"[^a-zA-Z0-9_-]+", "-", text.strip().lower()).strip("-_")
    return cleaned or "default"


def _default_agent_id_for_model(model_name: str) -> str:
    parts = model_name.split("/", 1)
    provider_part = _sanitize_agent_id_component(parts[0] if len(parts) > 1 else "openai")
    model_part = _sanitize_agent_id_component(parts[1] if len(parts) > 1 else parts[0])
    return f"hal-{provider_part}-{model_part}"[:80]


def _resolve_openclaw_agent_id(
    kwargs: dict, normalized_model_name: str, provider: str
) -> Optional[str]:
    by_model = _parse_mapping_kwarg(
        kwargs.get("openclaw_agent_by_model", kwargs.get("agent_by_model"))
    )
    by_provider = _parse_mapping_kwarg(
        kwargs.get("openclaw_agent_by_provider", kwargs.get("agent_by_provider"))
    )

    explicit_agent = (
        kwargs.get("openclaw_agent_id")
        or kwargs.get("agent_id")
        or kwargs.get("agent")
    )
    explicit_agent = str(explicit_agent).strip() if explicit_agent else ""

    model_key = normalized_model_name
    model_short = normalized_model_name.split("/", 1)[-1]

    if model_key in by_model:
        return by_model[model_key]
    if model_short in by_model:
        return by_model[model_short]
    if provider in by_provider:
        return by_provider[provider]

    if explicit_agent:
        per_model = kwargs.get("openclaw_agent_per_model", True)
        if not _as_bool(per_model, True):
            return explicit_agent
        suffix = _sanitize_agent_id_component(normalized_model_name.split("/", 1)[-1])
        return f"{explicit_agent}-{suffix}"[:80]

    auto_agent = kwargs.get("openclaw_auto_agent_from_model", True)
    if not _as_bool(auto_agent, True):
        return None
    return _default_agent_id_for_model(normalized_model_name)


def _scope_agent_id_for_question(
    agent_id: Optional[str], task_id: str, kwargs: dict
) -> Optional[str]:
    """
    Optional session isolation by question/task.
    We isolate by creating a task-scoped agent id, which maps to a separate
    OpenClaw main session.
    """
    if not agent_id:
        return agent_id
    separate = kwargs.get("openclaw_separate_session_per_question", True)
    if not _as_bool(separate, True):
        return agent_id
    task_suffix = _sanitize_agent_id_component(str(task_id))
    if not task_suffix:
        return agent_id

    scoped = f"{agent_id}-q-{task_suffix}"

    # Isolate reruns by default so repeated executions of the same task do not
    # inherit previous OpenClaw session history.
    isolate_reruns = kwargs.get("openclaw_isolate_reruns", True)
    if _as_bool(isolate_reruns, True):
        explicit_session = kwargs.get("openclaw_session_id")
        if explicit_session:
            run_suffix = _sanitize_agent_id_component(str(explicit_session))
        else:
            run_suffix = uuid.uuid4().hex[:8]
        if run_suffix:
            scoped = f"{scoped}-r-{run_suffix}"

    return scoped[:80]


def _is_already_exists_error(stdout: str, stderr: str) -> bool:
    text = f"{stdout}\n{stderr}".lower()
    return "already exists" in text or "duplicate" in text or "exists" in text


def _parse_json_loose(text: str) -> Optional[Any]:
    raw = (text or "").strip()
    if not raw:
        return None

    candidates = [raw]
    lines = [line.strip() for line in raw.splitlines() if line.strip()]
    if lines:
        candidates.append(lines[-1])

    for candidate in candidates:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            continue
    return None


def _find_agent_config_index(agents_payload: Any, agent_id: str) -> Optional[int]:
    agents_list = None
    if isinstance(agents_payload, list):
        agents_list = agents_payload
    elif isinstance(agents_payload, dict):
        if isinstance(agents_payload.get("agents"), list):
            agents_list = agents_payload["agents"]
        elif isinstance(agents_payload.get("list"), list):
            agents_list = agents_payload["list"]

    if not isinstance(agents_list, list):
        return None

    for idx, item in enumerate(agents_list):
        if not isinstance(item, dict):
            continue
        if str(item.get("id", "")).strip() == str(agent_id).strip():
            return idx
    return None


def _configure_agent_tool_policy(
    agent_id: str,
    clean_env: dict,
    *,
    disable_native_tools: bool,
) -> None:
    """
    Configure per-agent tool policy.

    For TauBench we disable all OpenClaw-native tools (`tools.deny=["*"]`)
    so tool execution happens only through the TauBench environment.
    """
    if not disable_native_tools:
        return
    if agent_id in _TOOL_POLICY_CONFIGURED_AGENT_IDS:
        return

    get_cmd = [
        "openclaw",
        "config",
        "get",
        "agents.list",
        "--json",
    ]
    get_result = subprocess.run(
        get_cmd,
        capture_output=True,
        text=True,
        timeout=30,
        env=clean_env,
    )
    if get_result.returncode != 0:
        raise RuntimeError(
            "Failed to read OpenClaw agent config while setting tool policy. "
            f"stderr: {(get_result.stderr or '').strip()[:300]}"
        )

    agents_payload = _parse_json_loose(get_result.stdout or "")
    agent_idx = _find_agent_config_index(agents_payload, agent_id)
    if agent_idx is None:
        raise RuntimeError(
            f"Could not locate agent '{agent_id}' in OpenClaw config after creation."
        )

    # Disable all OpenClaw-native tools for this agent.
    set_path = f"agents.list[{agent_idx}].tools"
    set_value = '{"deny":["*"]}'
    set_cmd = [
        "openclaw",
        "config",
        "set",
        set_path,
        set_value,
        "--strict-json",
    ]
    set_result = subprocess.run(
        set_cmd,
        capture_output=True,
        text=True,
        timeout=30,
        env=clean_env,
    )
    if set_result.returncode != 0:
        # Backward-compatible retry for older CLIs that may not support --strict-json.
        retry_cmd = [
            "openclaw",
            "config",
            "set",
            set_path,
            set_value,
        ]
        retry_result = subprocess.run(
            retry_cmd,
            capture_output=True,
            text=True,
            timeout=30,
            env=clean_env,
        )
        if retry_result.returncode != 0:
            raise RuntimeError(
                "Failed to disable OpenClaw-native tools for TauBench agent "
                f"'{agent_id}'. stderr: {(retry_result.stderr or '').strip()[:300]}"
            )

    _TOOL_POLICY_CONFIGURED_AGENT_IDS.add(agent_id)


def _seed_workspace_files(workspace: Path, agent_id: str) -> None:
    """
    Seed minimal workspace files for unattended runs.
    This avoids first-run bootstrap Q&A by ensuring bootstrap files exist.
    """
    today_str = date.today().isoformat()
    memory_dir = workspace / "memory"
    memory_dir.mkdir(parents=True, exist_ok=True)

    defaults = {
        "AGENTS.md": (
            "# AGENTS.md\n\n"
            "You are OpenClaw running as an autonomous agent under the HAL harness.\n"
            "Objective: solve the benchmark task and return the required final output format.\n"
            "Constraints:\n"
            "- No human intervention is available during benchmark runs.\n"
            "- Do not ask for confirmation or clarification; use best-effort assumptions and continue.\n"
            "- Keep output concise, factual, and policy-compliant.\n"
        ),
        "SOUL.md": (
            "# SOUL.md\n\n"
            "Identity: OpenClaw benchmark agent\n"
            "Execution environment: HAL harness\n"
            "Mode: fully autonomous, deterministic when possible, and safety-aware\n"
        ),
        "TOOLS.md": (
            "# TOOLS.md\n\n"
            "Use OpenClaw tools as needed to complete the task.\n"
            "Prefer verifiable and minimally destructive operations unless task requirements demand otherwise.\n"
        ),
        "USER.md": (
            "# USER.md\n\n"
            "User: HAL benchmark harness\n"
            "This is a programmatic run with no live human in the loop.\n"
            "Interaction style: concise and task-focused.\n"
        ),
        "IDENTITY.md": (
            "# IDENTITY.md\n\n"
            f"Name: hal-openclaw-{_sanitize_agent_id_component(agent_id)}\n"
            "System: OpenClaw\n"
            "Role: autonomous benchmark task-solving agent under HAL\n"
        ),
    }

    for fname, content in defaults.items():
        target = workspace / fname
        if not target.exists():
            target.write_text(content, encoding="utf-8")

    day_note = memory_dir / f"{today_str}.md"
    if not day_note.exists():
        day_note.write_text(
            f"# {today_str}\n\n"
            "Session memory for benchmark execution.\n",
            encoding="utf-8",
        )

    bootstrap_file = workspace / "BOOTSTRAP.md"
    if bootstrap_file.exists():
        try:
            bootstrap_file.unlink()
        except OSError:
            pass


def _ensure_openclaw_agent_configured(
    agent_id: Optional[str],
    normalized_model_name: str,
    clean_env: dict,
    kwargs: dict,
    *,
    disable_native_tools: bool = False,
) -> None:
    """
    Ensure the selected OpenClaw agent exists and is configured for the model.
    Uses non-interactive creation so benchmark runs need no manual setup.
    """
    if not agent_id:
        return
    if agent_id in _CONFIGURED_AGENT_IDS:
        _configure_agent_tool_policy(
            str(agent_id),
            clean_env,
            disable_native_tools=disable_native_tools,
        )
        return

    auto_configure = kwargs.get("openclaw_auto_configure_agents", True)
    if not _as_bool(auto_configure, True):
        return

    workspace_override = kwargs.get("openclaw_agent_workspace")
    if workspace_override:
        workspace = Path(str(workspace_override))
    else:
        workspace_base = Path(str(kwargs.get("openclaw_agent_workspace_base", "/tmp/openclaw-hal-agents")))
        workspace = workspace_base / _sanitize_agent_id_component(agent_id)
    workspace.mkdir(parents=True, exist_ok=True)
    _seed_workspace_files(workspace, agent_id)

    add_cmd = [
        "openclaw",
        "agents",
        "add",
        str(agent_id),
        "--workspace",
        str(workspace),
        "--model",
        normalized_model_name,
        "--non-interactive",
        "--json",
    ]
    result = subprocess.run(
        add_cmd,
        capture_output=True,
        text=True,
        timeout=90,
        env=clean_env,
    )

    if result.returncode == 0 or _is_already_exists_error(result.stdout or "", result.stderr or ""):
        _configure_agent_tool_policy(
            str(agent_id),
            clean_env,
            disable_native_tools=disable_native_tools,
        )
        _CONFIGURED_AGENT_IDS.add(agent_id)
        return

    raise RuntimeError(
        f"Failed to configure OpenClaw agent '{agent_id}' for model "
        f"'{normalized_model_name}'. stderr: {(result.stderr or '').strip()[:400]}"
    )


def _map_reasoning_to_thinking(reasoning_effort: Optional[str]) -> Optional[str]:
    if not reasoning_effort:
        return None
    mapped = {
        "xlow": "low",
        "low": "low",
        "medium": "medium",
        "high": "high",
        "xhigh": "max",
        "max": "max",
    }
    return mapped.get(str(reasoning_effort).lower(), str(reasoning_effort))


def _ensure_provider_key(provider: str) -> None:
    if provider == "openai" and not os.getenv("OPENAI_API_KEY"):
        raise RuntimeError(
            "OPENAI_API_KEY is required for provider=openai. "
            "See https://docs.openclaw.ai/providers/openai#option-a-openai-api-key-openai-platform"
        )
    if provider == "anthropic" and not os.getenv("ANTHROPIC_API_KEY"):
        raise RuntimeError(
            "ANTHROPIC_API_KEY is required for provider=anthropic. "
            "See https://docs.openclaw.ai/providers/anthropic#option-a-anthropic-api-key"
        )


def _maybe_auto_onboard(provider: str, clean_env: dict, kwargs: dict) -> None:
    """
    Optional one-time non-interactive onboarding so tasks can run unattended.
    Enabled by default; disable with -A openclaw_auto_onboard=false.
    """
    auto_onboard = kwargs.get("openclaw_auto_onboard", True)
    if not _as_bool(auto_onboard, True):
        return

    if provider in _ONBOARDED_PROVIDERS:
        return

    if provider == "openai":
        api_key = clean_env.get("OPENAI_API_KEY", "")
        auth_choice = "openai-api-key"
        key_flag = "--openai-api-key"
    else:
        api_key = clean_env.get("ANTHROPIC_API_KEY", "")
        auth_choice = "anthropic-api-key"
        key_flag = "--anthropic-api-key"

    if not api_key:
        return

    onboard_cmd = [
        "openclaw",
        "onboard",
        "--non-interactive",
        "--mode",
        "local",
        "--auth-choice",
        auth_choice,
        key_flag,
        api_key,
    ]

    try:
        onboard_result = subprocess.run(
            onboard_cmd,
            capture_output=True,
            text=True,
            timeout=60,
            env=clean_env,
        )
        if onboard_result.returncode == 0:
            _ONBOARDED_PROVIDERS.add(provider)
    except Exception:
        # Keep running without hard-failing onboarding; direct API-key auth may still work.
        pass


def _build_openclaw_base_cmd(
    agent_id: Optional[str], provider: str, reasoning_effort: Optional[str]
) -> list[str]:
    cmd = [
        "openclaw",
        "agent",
        "--local",
        "--json",
    ]
    if agent_id:
        cmd.extend(["--agent", str(agent_id)])

    thinking = _map_reasoning_to_thinking(reasoning_effort)
    if provider == "openai" and thinking:
        cmd.extend(["--thinking", thinking])
    return cmd


def _without_flag_pair(cmd: list[str], flag: str) -> list[str]:
    pruned = []
    i = 0
    while i < len(cmd):
        if cmd[i] == flag:
            i += 2
            continue
        pruned.append(cmd[i])
        i += 1
    return pruned


def _invoke_openclaw(
    base_cmd: list[str], prompt: str, clean_env: dict, timeout_seconds: int
) -> subprocess.CompletedProcess:
    """
    Run OpenClaw in non-interactive JSON mode.
    Retries without unsupported flags for older CLI versions.
    """
    cmd = base_cmd + ["--message", prompt]
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        timeout=timeout_seconds,
        env=clean_env,
    )

    stderr_lower = (result.stderr or "").lower()
    if result.returncode == 0:
        return result

    if "--thinking" in cmd and (
        "unknown option '--thinking'" in stderr_lower
        or "unknown argument '--thinking'" in stderr_lower
    ):
        retry_cmd = _without_flag_pair(base_cmd, "--thinking") + ["--message", prompt]
        result = subprocess.run(
            retry_cmd,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env=clean_env,
        )
        stderr_lower = (result.stderr or "").lower()
        if result.returncode == 0:
            return result

    return result


def _parse_openclaw_output(stdout: str, stderr: str = "") -> dict:
    """Parse `openclaw agent --json` output into a stable structure."""
    conversation_history = []
    taken_actions = []
    errors = []
    final_answer = ""
    cost_usd = 0.0
    total_input_tokens = 0
    total_output_tokens = 0

    def as_text(value: Any) -> str:
        if value is None:
            return ""
        if isinstance(value, str):
            return value
        if isinstance(value, list):
            parts = []
            for item in value:
                if isinstance(item, dict):
                    if isinstance(item.get("text"), str):
                        parts.append(item["text"])
                    elif isinstance(item.get("content"), str):
                        parts.append(item["content"])
                    elif isinstance(item.get("value"), str):
                        parts.append(item["value"])
                elif isinstance(item, str):
                    parts.append(item)
            return "\n".join(p for p in parts if p).strip()
        if isinstance(value, dict):
            for key in ("text", "content", "value", "output", "result", "reply"):
                candidate = value.get(key)
                if isinstance(candidate, str) and candidate.strip():
                    return candidate.strip()
            return json.dumps(value)
        return str(value)

    def parse_action(raw_action: dict) -> None:
        if not isinstance(raw_action, dict):
            return
        function_obj = raw_action.get("function")
        name = (
            raw_action.get("name")
            or raw_action.get("tool")
            or raw_action.get("tool_name")
            or raw_action.get("function_name")
            or ""
        )
        if not name and isinstance(function_obj, dict):
            name = function_obj.get("name") or ""
        kwargs = (
            raw_action.get("arguments")
            or raw_action.get("kwargs")
            or raw_action.get("input")
            or raw_action.get("args")
            or (function_obj.get("arguments") if isinstance(function_obj, dict) else None)
            or {}
        )
        if isinstance(kwargs, str):
            try:
                kwargs = json.loads(kwargs)
            except json.JSONDecodeError:
                return
        if kwargs is None:
            kwargs = {}
        if not isinstance(kwargs, dict):
            return
        if name:
            taken_actions.append({"name": str(name), "kwargs": kwargs})

    def parse_messages(messages: Any) -> None:
        if not isinstance(messages, list):
            return
        for msg in messages:
            if not isinstance(msg, dict):
                continue
            role = msg.get("role") or msg.get("type") or "assistant"
            content = as_text(msg.get("content", msg.get("text", msg.get("message", ""))))
            if content:
                conversation_history.append({"role": str(role), "content": content})

    # Attempt 1: single JSON object
    parsed_json: Optional[Any] = None
    try:
        parsed_json = json.loads(stdout)
    except (json.JSONDecodeError, TypeError):
        pass

    # Attempt 2: JSONL stream
    if parsed_json is None:
        events = []
        for line in stdout.strip().split("\n"):
            if not line.strip():
                continue
            try:
                events.append(json.loads(line))
            except json.JSONDecodeError:
                continue
        if events:
            parsed_json = {"events": events}

    if isinstance(parsed_json, dict):
        direct_answer = as_text(
            parsed_json.get("reply")
            or parsed_json.get("result")
            or parsed_json.get("output")
            or parsed_json.get("content")
            or parsed_json.get("text")
        )
        if direct_answer:
            final_answer = direct_answer

        parse_messages(
            parsed_json.get("messages")
            or parsed_json.get("conversation")
            or parsed_json.get("history")
        )

        for action in parsed_json.get("tool_calls", []):
            parse_action(action)
        for action in parsed_json.get("actions", []):
            parse_action(action)

        if isinstance(parsed_json.get("usage"), dict):
            usage = parsed_json["usage"]
            total_input_tokens += int(
                usage.get("input_tokens", usage.get("prompt_tokens", 0)) or 0
            )
            total_output_tokens += int(
                usage.get("output_tokens", usage.get("completion_tokens", 0)) or 0
            )
        if "cost_usd" in parsed_json:
            cost_usd = float(parsed_json.get("cost_usd") or 0.0)
        elif "cost" in parsed_json:
            try:
                cost_usd = float(parsed_json.get("cost") or 0.0)
            except (TypeError, ValueError):
                cost_usd = 0.0

        events = parsed_json.get("events", [])
        if isinstance(events, list):
            for event in events:
                if not isinstance(event, dict):
                    continue
                etype = str(event.get("type", ""))
                if "error" in etype:
                    errors.append(as_text(event.get("error") or event.get("message")))

                event_msg = (
                    event.get("message")
                    or event.get("assistant_message")
                    or event.get("response")
                )
                if isinstance(event_msg, dict):
                    role = event_msg.get("role", "assistant")
                    content = as_text(
                        event_msg.get("content", event_msg.get("text", event_msg))
                    )
                    if content:
                        conversation_history.append({"role": str(role), "content": content})
                elif isinstance(event_msg, str) and event_msg.strip():
                    conversation_history.append({"role": "assistant", "content": event_msg.strip()})

                payloads = [event]
                event_data = event.get("data")
                if isinstance(event_data, dict):
                    payloads.append(event_data)

                for payload in payloads:
                    for key in ("tool_call", "toolCall", "action"):
                        raw = payload.get(key)
                        if isinstance(raw, dict):
                            parse_action(raw)
                    for key in ("tool_calls", "toolCalls", "actions"):
                        raw_list = payload.get(key)
                        if isinstance(raw_list, list):
                            for raw in raw_list:
                                parse_action(raw)

                usage = event.get("usage")
                if isinstance(usage, dict):
                    total_input_tokens += int(
                        usage.get("input_tokens", usage.get("prompt_tokens", 0)) or 0
                    )
                    total_output_tokens += int(
                        usage.get("output_tokens", usage.get("completion_tokens", 0)) or 0
                    )

    if not final_answer and conversation_history:
        for msg in reversed(conversation_history):
            if msg.get("role") == "assistant" and msg.get("content", "").strip():
                final_answer = msg["content"].strip()
                break

    if not final_answer:
        # Plain-text fallback (works if --json is unavailable in older versions).
        final_answer = (stdout or "").strip()
        if final_answer:
            conversation_history.append({"role": "assistant", "content": final_answer})

    if stderr and not errors:
        # Keep stderr for diagnostics; don't force it into final answer.
        errors.append(stderr.strip()[:500])

    return {
        "conversation_history": conversation_history,
        "taken_actions": taken_actions,
        "final_answer": final_answer,
        "cost_usd": cost_usd,
        "errors": [e for e in errors if e],
        "total_input_tokens": total_input_tokens,
        "total_output_tokens": total_output_tokens,
    }


# =============================================================================
# TOOL FORMATTING AND EXTRACTION (for taubench multi-turn loop)
# =============================================================================


def _format_tools_for_prompt(tools_info: list) -> str:
    """Format taubench tool definitions as human-readable descriptions for the prompt."""
    lines = []
    for tool_def in tools_info:
        func = tool_def.get("function", tool_def)
        name = func.get("name", "unknown")
        desc = func.get("description", "")
        params = func.get("parameters", {}).get("properties", {})
        required = func.get("parameters", {}).get("required", [])

        lines.append(f"\n### {name}")
        lines.append(f"  {desc}")
        if params:
            lines.append("  Parameters:")
            for pname, pinfo in params.items():
                req = " (REQUIRED)" if pname in required else ""
                ptype = pinfo.get("type", "string")
                pdesc = pinfo.get("description", "")
                if pinfo.get("enum"):
                    pdesc += f" Options: {pinfo['enum']}"
                lines.append(f"    - {pname} ({ptype}){req}: {pdesc}")
    return "\n".join(lines)


def _extract_tool_call(text: str) -> Optional[dict]:
    """
    Extract a tool call from model text.

    Looks for patterns like:
      TOOL_CALL: {"name": "...", "kwargs": {...}}
    Also handles JSON in code blocks or standalone JSON objects.
    """
    if not text:
        return None

    # Strategy 1: Find TOOL_CALL: prefix and parse the JSON after it
    match = re.search(r"TOOL_CALL:\s*(\{.*)", text, re.DOTALL)
    if match:
        json_str = match.group(1)
        # Find the matching closing brace
        depth = 0
        for i, ch in enumerate(json_str):
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    try:
                        call = json.loads(json_str[: i + 1])
                        if "name" in call:
                            if "kwargs" not in call:
                                call["kwargs"] = {}
                            return call
                    except json.JSONDecodeError:
                        pass
                    break

        # If we reached the end of string with unclosed braces (e.g. model
        # dropped trailing '}'), try appending the missing braces.
        if depth > 0:
            repaired = json_str.rstrip() + "}" * depth
            try:
                call = json.loads(repaired)
                if isinstance(call, dict) and "name" in call:
                    if "kwargs" not in call:
                        call["kwargs"] = {}
                    return call
            except json.JSONDecodeError:
                pass

    # Strategy 2: Find JSON in code blocks
    for match in re.finditer(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL):
        try:
            call = json.loads(match.group(1))
            if "name" in call:
                if "kwargs" not in call:
                    call["kwargs"] = {}
                return call
        except json.JSONDecodeError:
            continue

    # Strategy 3: Parse the entire text as JSON
    try:
        call = json.loads(text.strip())
        if isinstance(call, dict) and "name" in call:
            if "kwargs" not in call:
                call["kwargs"] = {}
            return call
    except json.JSONDecodeError:
        pass

    # Strategy 4: Find any JSON object containing "name" key
    for match in re.finditer(r"\{", text):
        start = match.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == "{":
                depth += 1
            elif text[i] == "}":
                depth -= 1
                if depth == 0:
                    try:
                        call = json.loads(text[start : i + 1])
                        if isinstance(call, dict) and "name" in call:
                            if "kwargs" not in call:
                                call["kwargs"] = {}
                            return call
                    except json.JSONDecodeError:
                        pass
                    break
        # Try repairing missing closing braces
        if depth > 0:
            repaired = text[start:].rstrip() + "}" * depth
            try:
                call = json.loads(repaired)
                if isinstance(call, dict) and "name" in call:
                    if "kwargs" not in call:
                        call["kwargs"] = {}
                    return call
            except json.JSONDecodeError:
                pass

    return None


def _normalize_tool_call_candidate(raw: Any) -> Optional[dict]:
    if not isinstance(raw, dict):
        return None

    name = (
        raw.get("name")
        or raw.get("tool")
        or raw.get("tool_name")
        or raw.get("function_name")
        or ""
    )
    name = str(name).strip()
    if not name:
        return None

    kwargs = raw.get("kwargs", raw.get("arguments", raw.get("input", raw.get("args"))))
    if kwargs is None:
        kwargs = {}
    if isinstance(kwargs, str):
        try:
            kwargs = json.loads(kwargs)
        except json.JSONDecodeError:
            return None
    if not isinstance(kwargs, dict):
        return None

    return {"name": name, "kwargs": kwargs}


def _tool_call_signature(tool_call: dict) -> str:
    kwargs = tool_call.get("kwargs", {})
    try:
        kwargs_str = json.dumps(
            kwargs,
            sort_keys=True,
            ensure_ascii=True,
            separators=(",", ":"),
        )
    except (TypeError, ValueError):
        kwargs_str = str(kwargs)
    return f"{tool_call.get('name', '')}|{kwargs_str}"


def _collect_structured_tool_calls(parsed: dict) -> list[dict]:
    raw_actions = parsed.get("taken_actions")
    if not isinstance(raw_actions, list):
        return []

    unique_calls = []
    seen = set()
    for raw in raw_actions:
        candidate = _normalize_tool_call_candidate(raw)
        if not candidate:
            continue
        signature = _tool_call_signature(candidate)
        if signature in seen:
            continue
        seen.add(signature)
        unique_calls.append(candidate)
    return unique_calls


def _extract_allowed_tool_names(tools_info: list) -> set[str]:
    names = set()
    for tool_def in tools_info:
        if not isinstance(tool_def, dict):
            continue
        func = tool_def.get("function", tool_def)
        if not isinstance(func, dict):
            continue
        name = func.get("name")
        if isinstance(name, str) and name.strip():
            names.add(name.strip())
    return names


def _validate_tool_call(
    tool_call: dict, allowed_tool_names: set[str], validate_tool_names: bool
) -> Tuple[bool, str]:
    candidate = _normalize_tool_call_candidate(tool_call)
    if not candidate:
        return False, "invalid_shape"
    if (
        validate_tool_names
        and allowed_tool_names
        and candidate["name"] not in allowed_tool_names
    ):
        return False, f"unknown_tool:{candidate['name']}"
    return True, "ok"


def _select_taubench_tool_call(
    parsed: dict,
    response_text: str,
    allowed_tool_names: set[str],
    kwargs: dict,
) -> Tuple[Optional[dict], str, str]:
    """
    Deterministic tool-call selection:
    1) structured actions from --json output (preferred)
    2) optional text fallback (TOOL_CALL parsing)
    """
    allow_text_fallback = _as_bool(
        kwargs.get("openclaw_allow_text_tool_call_fallback"), False
    )
    require_single_structured = _as_bool(
        kwargs.get("openclaw_require_single_structured_tool_call"), True
    )
    validate_tool_names = _as_bool(kwargs.get("openclaw_validate_tool_names"), True)

    structured_candidates = _collect_structured_tool_calls(parsed)
    valid_structured = []
    structured_errors = []
    for candidate in structured_candidates:
        ok, reason = _validate_tool_call(
            candidate, allowed_tool_names, validate_tool_names
        )
        if ok:
            valid_structured.append(candidate)
        else:
            structured_errors.append(reason)

    if valid_structured:
        if require_single_structured and len(valid_structured) != 1:
            return (
                None,
                "none",
                f"multiple_structured_tool_calls:{len(valid_structured)}",
            )
        selected = valid_structured[0] if require_single_structured else valid_structured[-1]
        return selected, "structured", "ok"

    if structured_candidates and structured_errors:
        return None, "none", f"invalid_structured_tool_call:{structured_errors[0]}"

    if not allow_text_fallback:
        return None, "none", "missing_structured_tool_call"

    text_call = _extract_tool_call(response_text)
    if not text_call:
        return None, "none", "missing_text_tool_call"
    ok, reason = _validate_tool_call(text_call, allowed_tool_names, validate_tool_names)
    if not ok:
        return None, "none", f"invalid_text_tool_call:{reason}"
    return text_call, "text", "ok"


# =============================================================================
# CONFIDENCE SCORING (adapted from tool_calling scaffold)
# =============================================================================


def _compute_confidence_score(
    model_name: str,
    conversation_history: list,
    reward: float,
    actions_taken: list,
    system_context: str,
    initial_observation: str,
    base_cmd: list,
    clean_env: dict,
) -> tuple:
    """
    Compute confidence score via self-assessment using OpenClaw.

    Appends a confidence-assessment message to the conversation history and
    invokes OpenClaw one more time (same subprocess approach as the
    multi-turn loop), then parses the numeric response.

    Returns:
        (confidence_score: float, confidence_details: dict)
    """
    num_errors = sum(
        1
        for a in actions_taken
        if isinstance(a, dict) and "Error" in str(a.get("observation", ""))
    )
    num_actions = len(actions_taken)

    confidence_question = (
        "You just completed a customer service task. Please assess your confidence "
        "in the correctness of your solution.\n\n"
        "Consider the following when rating your confidence:\n"
        "1. Did you encounter any errors or failures during execution?\n"
        "2. Were you able to complete all necessary steps?\n"
        "3. How clear and unambiguous was the task?\n"
        "4. How reliable was the information and tools you used?\n"
        "5. Are there any aspects where you're uncertain?\n\n"
        "Please provide a confidence score from 0 to 100, where:\n"
        "- 0-20: Very uncertain, likely incorrect\n"
        "- 21-40: Low confidence, significant doubts\n"
        "- 41-60: Moderate confidence, some uncertainty\n"
        "- 61-80: Good confidence, minor doubts\n"
        "- 81-100: Very confident, highly certain of correctness\n\n"
        "Respond with ONLY a number between 0 and 100. No explanation needed."
    )

    # Build a prompt identical to the turn loop format, with the confidence
    # question appended as a final user message.
    prompt = system_context + "\n\n"
    prompt += "=" * 50 + "\n"
    prompt += "CONVERSATION SO FAR:\n"
    prompt += "=" * 50 + "\n\n"

    prompt += f"[CUSTOMER]: {initial_observation}\n\n"

    for entry in conversation_history:
        role = entry["role"]
        content = entry["content"]
        if role == "assistant":
            prompt += f"[YOU]: {content}\n\n"
        elif role == "tool_result":
            prompt += f"[TOOL RESULT]: {content}\n\n"
        elif role == "customer":
            prompt += f"[CUSTOMER]: {content}\n\n"
        elif role == "system":
            prompt += f"[SYSTEM]: {content}\n\n"

    prompt += "=" * 50 + "\n"
    prompt += "CONFIDENCE ASSESSMENT:\n"
    prompt += "=" * 50 + "\n\n"
    prompt += confidence_question

    try:
        print(f"  Confidence assessment: invoking OpenClaw for {model_name}")

        proc_result = _invoke_openclaw(
            base_cmd=base_cmd,
            prompt=prompt,
            clean_env=clean_env,
            timeout_seconds=120,
        )

        parsed = _parse_openclaw_output(proc_result.stdout or "", proc_result.stderr or "")
        response_text = parsed.get("final_answer", "").strip()

        if not response_text:
            raise ValueError("Empty response from OpenClaw for confidence assessment")

        numbers = re.findall(r"\d+", response_text)

        if numbers:
            confidence_score = float(numbers[0]) / 100.0
            confidence_score = max(0.0, min(1.0, confidence_score))
            print(f"  Confidence: '{response_text}' -> {confidence_score:.2f}")
        else:
            print(
                f"  Warning: Could not parse confidence from '{response_text}', using default 0.5"
            )
            confidence_score = 0.5

        confidence_details = {
            "prompt": confidence_question,
            "model_response": response_text,
            "parsed_score": confidence_score,
            "num_actions": num_actions,
            "num_errors": num_errors,
            "task_reward": reward,
            "model": model_name,
            "cost_usd": parsed.get("cost_usd", 0.0),
        }

        return confidence_score, confidence_details

    except Exception as e:
        print(f"  Warning: Error computing confidence score: {e}")
        if num_actions == 0:
            heuristic_confidence = 0.1
        else:
            error_penalty = num_errors / max(num_actions, 1)
            heuristic_confidence = max(0.1, 0.9 - error_penalty)

        heuristic_details = {
            "prompt": confidence_question,
            "model_response": str(e),
            "parsed_score": heuristic_confidence,
            "num_actions": num_actions,
            "num_errors": num_errors,
            "task_reward": reward,
            "model": model_name,
            "fallback": True,
        }
        return heuristic_confidence, heuristic_details


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================


def run(input: dict[str, dict], **kwargs) -> dict[str, str]:
    """
    HAL agent interface: run OpenClaw on a benchmark task.

    Both taubench and GAIA use the OpenClaw scaffold, preserving
    its agentic reasoning capabilities.
    """
    if not shutil.which("openclaw"):
        raise RuntimeError(
            "OpenClaw CLI not found. Install it, then verify `openclaw agent --help` works."
        )

    benchmark_name = kwargs.get("benchmark_name", "")

    if "taubench" in benchmark_name:
        return _run_taubench(input, **kwargs)
    else:
        return _run_gaia(input, **kwargs)


# =============================================================================
# GAIA BENCHMARK (single OpenClaw invocation)
# =============================================================================


def _run_gaia(input: dict, **kwargs) -> dict:
    """Run OpenClaw on a GAIA task."""
    task_id = list(input.keys())[0]
    task_data = input[task_id]

    question = task_data.get("Question", task_data.get("question", ""))
    file_name = task_data.get("file_name", "")

    prompt = (
        "You are solving a GAIA benchmark task. "
        "Answer the following question as accurately and concisely as possible.\n\n"
        f"Question: {question}\n"
    )

    if file_name and os.path.exists(file_name):
        prompt += f"\nA file has been provided at: {os.path.abspath(file_name)}\n"

    prompt += (
        "\nIMPORTANT: Your final answer must be precise and concise. "
        "After completing your research, state ONLY the final answer with no extra explanation.\n"
        "Do not ask for human confirmation; proceed autonomously."
    )

    model_name = str(kwargs.get("model_name", "gpt-5.1-codex"))
    provider = _normalize_provider(kwargs.get("provider"), model_name)
    _ensure_provider_key(provider)
    normalized_model_name = _normalize_model_name(provider, model_name)
    agent_id = _resolve_openclaw_agent_id(kwargs, normalized_model_name, provider)
    agent_id = _scope_agent_id_for_question(agent_id, task_id, kwargs)
    reasoning_effort = kwargs.get("reasoning_effort")
    clean_env = dict(os.environ)
    _maybe_auto_onboard(provider, clean_env, kwargs)
    _ensure_openclaw_agent_configured(
        agent_id,
        normalized_model_name,
        clean_env,
        kwargs,
        disable_native_tools=_as_bool(kwargs.get("openclaw_disable_native_tools"), False),
    )
    base_cmd = _build_openclaw_base_cmd(agent_id, provider, reasoning_effort)
    per_call_timeout = int(kwargs.get("openclaw_timeout_seconds", 600))

    start_time = time.time()
    try:
        result = _invoke_openclaw(
            base_cmd=base_cmd,
            prompt=prompt,
            clean_env=clean_env,
            timeout_seconds=per_call_timeout,
        )
        duration = time.time() - start_time

        if result.returncode != 0:
            print(f"OpenClaw exited with code {result.returncode}")
            if result.stderr:
                print(f"OpenClaw stderr: {result.stderr[:500]}")

        parsed = _parse_openclaw_output(result.stdout or "", result.stderr or "")
    except subprocess.TimeoutExpired:
        duration = time.time() - start_time
        parsed = {
            "conversation_history": [],
            "taken_actions": [],
            "final_answer": "",
            "cost_usd": 0.0,
        }
    except Exception as e:
        duration = time.time() - start_time
        parsed = {
            "conversation_history": [{"role": "system", "content": f"Error: {e}"}],
            "taken_actions": [],
            "final_answer": "",
            "cost_usd": 0.0,
        }

    abstention = _detect_abstention(
        parsed["conversation_history"], parsed["taken_actions"]
    )

    result_data = {
        "model_answer": parsed["final_answer"],
        "conversation_history": parsed["conversation_history"],
        "taken_actions": parsed["taken_actions"],
        "abstention": abstention,
        "duration_seconds": duration,
        "cost_usd": parsed["cost_usd"],
    }

    return {task_id: json.dumps(result_data)}


# =============================================================================
# TAUBENCH BENCHMARK (multi-turn OpenClaw loop)
#
# Each turn: build prompt with full conversation history, invoke OpenClaw once,
# extract tool call from response, execute against taubench env.
# =============================================================================


def _run_taubench(input: dict, **kwargs) -> dict:
    """Run OpenClaw on a taubench task via multi-turn tool calling loop.

    Improvements over basic CLI approach:
    - Structured conversation format with clear role labels
    - Allows reasoning before tool calls (leverages agentic model behavior)
    - Includes today's date to prevent date confusion
    - Distinguishes customer messages from tool results
    - Gentle nudging on consecutive non-tool-call responses
    """
    from tau_bench.envs import get_env
    from tau_bench.types import Action

    task_id = list(input.keys())[0]
    task_data = input[task_id]

    # Set up taubench environment
    isolated_env = get_env(
        task_data["env"],
        task_data["user_strategy"],
        task_data["user_model"],
        task_data["task_split"],
        task_data.get("user_provider", "openai"),
        task_data["task_index"],
    )

    # ========== RELIABILITY METRICS INITIALIZATION ==========

    # Initialize FaultInjector if enabled
    fault_injector: Optional[FaultInjector] = None
    if (
        kwargs.get("enable_fault_injection") == "true"
        or kwargs.get("enable_fault_injection") is True
    ):
        fault_rate = float(kwargs.get("fault_rate", 0.2))
        fault_injector = FaultInjector(fault_rate=fault_rate)
        print(f"  Fault injection enabled with rate={fault_rate}")

    # Initialize ComplianceMonitor if enabled
    compliance_monitor: Optional[ComplianceMonitor] = None
    if (
        kwargs.get("enable_compliance_monitoring") == "true"
        or kwargs.get("enable_compliance_monitoring") is True
    ):
        constraints_input = kwargs.get(
            "compliance_constraints", "no_pii_exposure,no_destructive_ops"
        )
        if isinstance(constraints_input, list):
            constraints = [c.strip() for c in constraints_input if c.strip()]
        else:
            constraints = [
                c.strip() for c in str(constraints_input).split(",") if c.strip()
            ]
        compliance_monitor = ComplianceMonitor(constraints=constraints)
        print(f"  Compliance monitoring enabled with constraints: {constraints}")

    # Initialize TauBench-specific StructuralPerturbator if enabled
    taubench_perturbator: Optional[TauBenchPerturbator] = None
    param_mapping: Dict[str, Dict[str, str]] = {}

    if (
        kwargs.get("enable_structural_perturbations") == "true"
        or kwargs.get("enable_structural_perturbations") is True
    ):
        perturbation_strength = kwargs.get("perturbation_strength", "medium")
        taubench_perturbator = create_taubench_perturbator(
            strength=perturbation_strength
        )
        print(
            f"  Tau-bench structural perturbations enabled: strength={perturbation_strength}"
        )
        print(
            f"   Config: key_case={taubench_perturbator.config.key_case}, "
            f"time_format={taubench_perturbator.config.time_format}, "
            f"param_style={taubench_perturbator.config.param_name_style}"
        )

    # Support prompt sensitivity: override instruction if provided
    if "instruction" in task_data and task_data["instruction"] is not None:
        isolated_env.task.instruction = task_data["instruction"]
        print(f"  Using custom instruction for task {task_id}")

    # Support for prompt variation style: Inject style directive into user's system prompt
    if "prompt_variation_strength" in task_data:
        from hal.utils.prompt_variation import get_user_style_directive

        style_strength = task_data["prompt_variation_strength"]
        style_directive = get_user_style_directive(style_strength)

        if style_directive:
            original_build_system_prompt = isolated_env.user.build_system_prompt

            def styled_build_system_prompt(instruction=None):
                base_prompt = original_build_system_prompt(instruction)
                return base_prompt + style_directive

            isolated_env.user.build_system_prompt = styled_build_system_prompt

            if hasattr(isolated_env.user, "messages") and isolated_env.user.messages:
                for msg in isolated_env.user.messages:
                    if msg.get("role") == "system":
                        msg["content"] = msg["content"] + style_directive
                        break

            print(f"  User communication style set to: {style_strength}")

    # Get initial observation (customer's first message)
    reset_response = isolated_env.reset(task_data["task_index"])
    initial_observation = reset_response.observation

    # ========== STRUCTURAL PERTURBATIONS (OPTIONAL) ==========
    perturbed_tools_info = isolated_env.tools_info
    perturbed_wiki = isolated_env.wiki

    if taubench_perturbator:
        # Perturb tool definitions (including parameter names)
        perturbed_tools_info, param_mapping = (
            taubench_perturbator.perturb_tool_definitions(isolated_env.tools_info)
        )

        # Perturb the wiki (knowledge base)
        perturbed_wiki = taubench_perturbator.perturb_tool_response(isolated_env.wiki)

        # Wrap the environment's step function to perturb tool responses
        # and reverse parameter name mapping
        original_step = isolated_env.step

        def perturbed_step(action):
            """Intercept tool calls to reverse param names and perturb responses."""
            if hasattr(action, "kwargs") and hasattr(action, "name"):
                if action.name in param_mapping:
                    original_kwargs = taubench_perturbator.reverse_param_mapping(
                        action.name, action.kwargs, param_mapping
                    )
                    action.kwargs = original_kwargs

            result = original_step(action)

            if (
                hasattr(result, "observation")
                and result.observation
                and isinstance(result.observation, str)
            ):
                result.observation = taubench_perturbator.perturb_tool_response(
                    result.observation
                )

            return result

        isolated_env.step = perturbed_step

        summary = taubench_perturbator.get_perturbation_summary()
        print(f"  Applied {summary['total_perturbations']} structural perturbations")
        if summary["by_type"]:
            print(f"   By type: {summary['by_type']}")

    # Format tool descriptions for prompt (using perturbed tools if applicable)
    tools_description = _format_tools_for_prompt(perturbed_tools_info)
    allowed_tool_names = _extract_allowed_tool_names(perturbed_tools_info)

    # Today's date (prevents date confusion  a known failure mode)
    today = date.today().strftime("%B %d, %Y")

    # System context included in every turn's prompt (using perturbed wiki if applicable)
    system_context = f"""You are a customer service agent helping a customer. Today's date is {today}.
Follow the company policies strictly.

COMPANY POLICIES AND KNOWLEDGE BASE:
{perturbed_wiki}

AVAILABLE TOOLS:
{tools_description}

RULES:
1. Look up customer information with tools BEFORE making any changes.
2. Verify customer identity before modifying their data.
3. Follow all policies from the knowledge base above strictly.
4. Do not make up information. Only use data returned by tools.
5. To send a message to the customer, use the "respond" tool with a "content" parameter.
6. Use the EXACT parameter names listed above for each tool.
7. Think step by step about what action to take next.
8. Do not ask for human confirmation; continue autonomously."""

    model_name = str(kwargs.get("model_name", "gpt-5.1-codex"))
    provider = _normalize_provider(kwargs.get("provider"), model_name)
    _ensure_provider_key(provider)
    normalized_model_name = _normalize_model_name(provider, model_name)
    agent_id = _resolve_openclaw_agent_id(kwargs, normalized_model_name, provider)
    agent_id = _scope_agent_id_for_question(agent_id, task_id, kwargs)
    max_turns = int(kwargs.get("max_turns", 30))
    reasoning_effort = kwargs.get("reasoning_effort")
    clean_env = dict(os.environ)
    _maybe_auto_onboard(provider, clean_env, kwargs)
    _ensure_openclaw_agent_configured(
        agent_id,
        normalized_model_name,
        clean_env,
        kwargs,
        disable_native_tools=_as_bool(
            kwargs.get("openclaw_disable_native_tools_for_taubench"), True
        ),
    )

    # Build base command (reused each turn, prompt appended per-turn)
    base_cmd = _build_openclaw_base_cmd(agent_id, provider, reasoning_effort)
    if reasoning_effort:
        print(f"  Reasoning effort: {reasoning_effort}")

    per_call_timeout = int(kwargs.get("openclaw_timeout_seconds", 120))

    # Conversation state
    conversation_history = []
    actions_log = []
    total_cost = 0.0
    done = False
    consecutive_no_tool_call = 0
    tool_call_source_counts = {"structured": 0, "text": 0}

    start_time = time.time()

    for turn in range(max_turns):
        if done:
            break

        # ---- Build prompt for this turn ----
        prompt = system_context + "\n\n"
        prompt += "=" * 50 + "\n"
        prompt += "CONVERSATION SO FAR:\n"
        prompt += "=" * 50 + "\n\n"

        # Initial customer message
        prompt += f"[CUSTOMER]: {initial_observation}\n\n"

        # Previous turns with clear role labels
        for entry in conversation_history:
            role = entry["role"]
            content = entry["content"]
            if role == "assistant":
                prompt += f"[YOU]: {content}\n\n"
            elif role == "tool_result":
                prompt += f"[TOOL RESULT]: {content}\n\n"
            elif role == "customer":
                prompt += f"[CUSTOMER]: {content}\n\n"
            elif role == "system":
                prompt += f"[SYSTEM]: {content}\n\n"

        # ---- Turn instruction ----
        prompt += "=" * 50 + "\n"
        prompt += "YOUR TURN:\n"
        prompt += "=" * 50 + "\n\n"

        # Add nudge if previous turns failed to produce tool calls
        if consecutive_no_tool_call > 0:
            prompt += (
                "IMPORTANT: Your previous response did not contain a valid TOOL_CALL. "
                "You MUST include a TOOL_CALL line to take action or respond to the customer.\n\n"
            )

        prompt += (
            "Think about what you need to do next, then take action.\n\n"
            "To call a tool, include this line in your response:\n"
            'TOOL_CALL: {"name": "tool_name", "kwargs": {"param1": "value1", "param2": "value2"}}\n\n'
            "To respond to the customer:\n"
            'TOOL_CALL: {"name": "respond", "kwargs": {"content": "Your message to the customer"}}\n\n'
            "You may include brief reasoning before the TOOL_CALL line, "
            "but you MUST include exactly one TOOL_CALL line in your response."
        )

        # ---- Invoke OpenClaw ----
        try:
            proc_result = _invoke_openclaw(
                base_cmd=base_cmd,
                prompt=prompt,
                clean_env=clean_env,
                timeout_seconds=per_call_timeout,
            )

            parsed = _parse_openclaw_output(
                proc_result.stdout or "", proc_result.stderr or ""
            )
            total_cost += parsed.get("cost_usd", 0.0)

            if proc_result.returncode != 0:
                print(
                    f"  Turn {turn}: OpenClaw exited with code {proc_result.returncode}"
                )
                if proc_result.stderr:
                    print(f"  OpenClaw stderr: {proc_result.stderr[:300]}")

            # Get the response text
            response_text = parsed.get("final_answer", "")
            tool_call, tool_call_source, tool_call_reason = _select_taubench_tool_call(
                parsed=parsed,
                response_text=response_text,
                allowed_tool_names=allowed_tool_names,
                kwargs=kwargs,
            )

            if not response_text and not tool_call:
                print(f"  Turn {turn}: Empty response from OpenClaw")
                consecutive_no_tool_call += 1
                if consecutive_no_tool_call >= 3:
                    print(
                        f"  Stopping: {consecutive_no_tool_call} consecutive empty/non-tool responses"
                    )
                    break
                continue

            if response_text:
                print(
                    f"  Turn {turn}: Response ({len(response_text)} chars): {response_text[:150]}"
                )
            elif tool_call:
                print(
                    f"  Turn {turn}: Structured tool call received without assistant text"
                )

            if tool_call:
                consecutive_no_tool_call = 0
                if tool_call_source in tool_call_source_counts:
                    tool_call_source_counts[tool_call_source] += 1
                action_name = tool_call["name"]
                action_kwargs = tool_call.get("kwargs", {})

                # Execute against taubench environment
                action = Action(name=action_name, kwargs=action_kwargs)

                try:
                    # Use fault injection wrapper if enabled
                    if fault_injector and fault_injector.enabled:
                        env_response = fault_injector.wrap_call(
                            isolated_env.step, action
                        )
                    else:
                        env_response = isolated_env.step(action)

                    actions_log.append(
                        {
                            "name": action_name,
                            "kwargs": action_kwargs,
                            "observation": env_response.observation[:500]
                            if env_response.observation
                            else "",
                            "done": env_response.done,
                        }
                    )

                    # Record the tool call in conversation history
                    conversation_history.append(
                        {
                            "role": "assistant",
                            "content": f"TOOL_CALL: {json.dumps(tool_call)}",
                        }
                    )

                    # Record the result  distinguish customer replies from tool results
                    if action_name == "respond":
                        # After responding, the observation is the customer's next message
                        conversation_history.append(
                            {
                                "role": "customer",
                                "content": env_response.observation or "",
                            }
                        )
                    else:
                        conversation_history.append(
                            {
                                "role": "tool_result",
                                "content": env_response.observation or "",
                            }
                        )

                    print(
                        f"  Turn {turn}: Executed {action_name}, done={env_response.done}"
                    )

                    if env_response.done:
                        done = True

                except Exception as e:
                    print(f"  Turn {turn}: Tool execution error: {e}")
                    conversation_history.append(
                        {
                            "role": "assistant",
                            "content": f"TOOL_CALL: {json.dumps(tool_call)}",
                        }
                    )
                    conversation_history.append(
                        {
                            "role": "tool_result",
                            "content": f"Error: {e}",
                        }
                    )
            else:
                # No parseable tool call
                consecutive_no_tool_call += 1
                print(
                    f"  Turn {turn}: No deterministic tool call found "
                    f"({consecutive_no_tool_call} consecutive, reason={tool_call_reason})"
                )
                # Add the model's raw response to history so next turn sees it
                if response_text:
                    conversation_history.append(
                        {
                            "role": "assistant",
                            "content": response_text,
                        }
                    )
                else:
                    conversation_history.append(
                        {
                            "role": "system",
                            "content": (
                                "No deterministic tool call extracted from OpenClaw "
                                f"(reason={tool_call_reason})."
                            ),
                        }
                    )
                if consecutive_no_tool_call >= 3:
                    print(
                        f"  Stopping: {consecutive_no_tool_call} consecutive non-tool-call responses"
                    )
                    break

        except subprocess.TimeoutExpired:
            print(f"  Turn {turn}: OpenClaw timed out")
            conversation_history.append(
                {
                    "role": "system",
                    "content": "OpenClaw timed out on this turn",
                }
            )
            continue
        except Exception as e:
            print(f"  Turn {turn}: Exception: {e}")
            conversation_history.append(
                {
                    "role": "system",
                    "content": f"Error: {e}",
                }
            )
            break

    duration = time.time() - start_time

    # ========== POST-LOOP RELIABILITY METRICS ==========

    # Abstention detection (always computed  lightweight, rule-based)
    abstention = _detect_abstention(conversation_history, actions_log)
    if abstention["abstained"]:
        print(
            f"  Abstention detected: type={abstention['abstention_type']}, "
            f"strength={abstention['abstention_strength']:.2f}"
        )

    # Confidence scoring (optional)
    confidence = None
    confidence_details = None
    if (
        kwargs.get("compute_confidence") == "true"
        or kwargs.get("compute_confidence") is True
    ):
        confidence, confidence_details = _compute_confidence_score(
            model_name=model_name,
            conversation_history=conversation_history,
            reward=isolated_env.reward,
            actions_taken=actions_log,
            system_context=system_context,
            initial_observation=initial_observation,
            base_cmd=base_cmd,
            clean_env=clean_env,
        )
        # Add confidence turn cost to total
        if confidence_details and "cost_usd" in confidence_details:
            total_cost += confidence_details["cost_usd"]

    # Compliance checking (optional)
    compliance_violations = []
    if compliance_monitor:
        task_output_str = json.dumps(isolated_env.task.model_dump())
        actions_str = json.dumps(
            [action.model_dump() for action in isolated_env.actions]
        )

        if "no_pii_exposure" in compliance_monitor.constraints or any(
            "pii" in c for c in compliance_monitor.constraints
        ):
            constraint_name = next(
                (c for c in compliance_monitor.constraints if "pii" in c),
                "no_pii_exposure",
            )
            passed, violation = compliance_monitor.check_constraint(
                constraint_name,
                text=task_output_str,
                log_output=actions_str,
            )
            if not passed and violation:
                compliance_violations.append(violation.to_dict())
                print(f"  PII violation detected: {violation.description}")

        if "no_destructive_ops" in compliance_monitor.constraints or any(
            "destructive" in c for c in compliance_monitor.constraints
        ):
            constraint_name = next(
                (c for c in compliance_monitor.constraints if "destructive" in c),
                "no_destructive_ops",
            )
            for action in isolated_env.actions:
                action_str = (
                    str(action.model_dump())
                    if hasattr(action, "model_dump")
                    else str(action)
                )
                passed, violation = compliance_monitor.check_constraint(
                    constraint_name,
                    operation=action_str,
                )
                if not passed and violation:
                    compliance_violations.append(violation.to_dict())
                    print(f"  Destructive op violation: {violation.description}")

        print(
            f"  Compliance check complete: {len(compliance_violations)} violations found"
        )

    # LLM-based log analysis (optional)
    llm_compliance_result = None
    llm_recovery_result = None

    enable_llm_analysis = (
        kwargs.get("enable_llm_analysis") == "true"
        or kwargs.get("enable_llm_analysis") is True
    )
    if enable_llm_analysis:
        llm_analysis_model = kwargs.get("llm_analysis_model", "gpt-4o-mini")
        print(f"  Running LLM-based log analysis with {llm_analysis_model}...")

        try:
            llm_analyzer = LLMLogAnalyzer(
                model=llm_analysis_model, cache_responses=False
            )

            actions_list = [action.model_dump() for action in isolated_env.actions]

            if kwargs.get("llm_compliance") != "false":
                llm_constraints = kwargs.get(
                    "llm_constraints", "no_pii_exposure,no_destructive_ops"
                ).split(",")
                llm_constraints = [c.strip() for c in llm_constraints if c.strip()]

                llm_compliance_result = llm_analyzer.analyze_compliance(
                    conversation_history=conversation_history,
                    actions_taken=actions_list,
                    constraints=llm_constraints,
                )
                print(
                    f"   LLM Compliance: S_comp={llm_compliance_result.S_comp:.2f}, "
                    f"violations={len(llm_compliance_result.violations)}"
                )

            if kwargs.get("llm_recovery") != "false":
                llm_recovery_result = llm_analyzer.detect_recovery_behavior(
                    conversation_history=conversation_history,
                    actions_taken=actions_list,
                )
                print(
                    f"   LLM Recovery: V_heal={llm_recovery_result.V_heal:.2f}, "
                    f"errors={llm_recovery_result.total_errors_encountered}"
                )

        except Exception as e:
            print(f"  LLM analysis error: {e}")

    # ========== BUILD RESULT ==========

    result_data = {
        "reward": isolated_env.reward,
        "taken_actions": [action.model_dump() for action in isolated_env.actions],
        "task": isolated_env.task.model_dump(),
        "tool_call_extraction": {
            "structured_count": tool_call_source_counts["structured"],
            "text_fallback_count": tool_call_source_counts["text"],
            "allow_text_tool_call_fallback": _as_bool(
                kwargs.get("openclaw_allow_text_tool_call_fallback"), False
            ),
            "require_single_structured_tool_call": _as_bool(
                kwargs.get("openclaw_require_single_structured_tool_call"), True
            ),
            "validate_tool_names": _as_bool(
                kwargs.get("openclaw_validate_tool_names"), True
            ),
        },
    }

    # Add confidence if computed
    if confidence is not None:
        result_data["confidence"] = confidence

    if confidence_details is not None and (
        kwargs.get("store_confidence_details") == "true"
        or kwargs.get("store_confidence_details") is True
    ):
        result_data["confidence_details"] = confidence_details

    # Add abstention detection result (always computed)
    result_data["abstention"] = {
        "abstained": abstention["abstained"],
        "abstention_type": abstention["abstention_type"],
        "abstention_strength": abstention["abstention_strength"],
        "early_termination": abstention["early_termination"],
        "evidence": abstention["evidence"],
        "scores_by_type": abstention["scores_by_type"],
    }

    # Add fault injection metrics if enabled
    if fault_injector:
        fault_stats = fault_injector.get_stats()
        fault_events = [e.to_dict() for e in fault_injector.get_fault_events()]
        result_data["fault_injection"] = {
            "enabled": True,
            "fault_rate": fault_injector.fault_rate,
            "stats": fault_stats,
            "events": fault_events,
            "V_heal": fault_stats["recovery_rate"],
            "mean_recovery_time": fault_stats["mean_recovery_time"],
        }
        print(
            f"  Fault stats: {fault_stats['total_faults_injected']} faults, "
            f"{fault_stats['recovery_rate']:.1%} recovery rate"
        )

    # Add compliance metrics if enabled
    if compliance_monitor:
        result_data["compliance"] = {
            "enabled": True,
            "constraints": compliance_monitor.constraints,
            "violations": compliance_violations,
            "violation_count": len(compliance_violations),
            "S_comp": 1.0 - (1.0 if compliance_violations else 0.0),
        }

    # Add structural perturbation metrics if enabled
    if taubench_perturbator:
        summary = taubench_perturbator.get_perturbation_summary()
        result_data["structural_perturbation"] = {
            "enabled": True,
            "perturbation_type": "taubench",
            "perturbation_count": summary["total_perturbations"],
            "perturbations_by_type": summary["by_type"],
            "applied_perturbations": taubench_perturbator.applied_perturbations[:50],
            "config": {
                "key_case": taubench_perturbator.config.key_case,
                "time_format": taubench_perturbator.config.time_format,
                "date_format": taubench_perturbator.config.date_format,
                "status_format": taubench_perturbator.config.status_format,
                "cabin_format": taubench_perturbator.config.cabin_format,
                "param_name_style": taubench_perturbator.config.param_name_style,
                "wrap_responses": taubench_perturbator.config.wrap_responses,
                "use_abbreviations": taubench_perturbator.config.use_abbreviations,
            },
            "param_mapping": param_mapping,
        }

    # Add LLM-based analysis results if enabled
    if llm_compliance_result is not None:
        result_data["llm_compliance"] = {
            "enabled": True,
            "S_comp": llm_compliance_result.S_comp,
            "num_violations": len(llm_compliance_result.violations),
            "violations": [v.to_dict() for v in llm_compliance_result.violations],
            "analysis_model": llm_compliance_result.analysis_model,
        }

    if llm_recovery_result is not None:
        result_data["llm_recovery"] = {
            "enabled": True,
            "V_heal": llm_recovery_result.V_heal,
            "total_errors": llm_recovery_result.total_errors_encountered,
            "recoveries_attempted": llm_recovery_result.total_recoveries_attempted,
            "recoveries_successful": llm_recovery_result.successful_recoveries,
            "recovery_attempts": [
                r.to_dict() for r in llm_recovery_result.recovery_attempts
            ],
            "analysis_model": llm_recovery_result.analysis_model,
        }

    # Store conversation history for post-hoc analysis if requested
    if (
        kwargs.get("store_conversation_history") == "true"
        or kwargs.get("store_conversation_history") is True
    ):
        result_data["conversation_history"] = conversation_history

    # Always include these operational fields
    result_data["duration_seconds"] = duration
    result_data["cost_usd"] = total_cost
    result_data["bridge_actions"] = actions_log
    result_data["num_turns"] = len(actions_log)

    return {task_id: result_data}
